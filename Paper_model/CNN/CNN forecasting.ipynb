{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import sys\n",
    "from tensorflow.keras.utils import disable_interactive_logging\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pymannkendall import seasonal_test\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv1D, LSTM, Lambda, Dropout,Bidirectional\n",
    "from keras_tuner import RandomSearch\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_tuner import HyperParameters\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_train(training_ratio,look_back,batch_size,file):\n",
    "    disable_interactive_logging()\n",
    "    class Tee:\n",
    "        def __init__(self, *files):\n",
    "            self.files = files\n",
    "        def write(self, obj):\n",
    "            for f in self.files:\n",
    "                f.write(obj)\n",
    "                f.flush()\n",
    "        def flush(self):\n",
    "            for f in self.files:\n",
    "                f.flush()\n",
    "\n",
    "    sys.stdout = Tee(sys.stdout, sys.stderr)\n",
    "\n",
    "    directory=f'lstm_{training_ratio}_{look_back}_{batch_size}'\n",
    "    if os.path.exists(directory):    \n",
    "        shutil.rmtree(directory)\n",
    "        print(f\"{directory} is removed successfully\")\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # load a dataset\n",
    "    data_file = f\"../Data/WekaData/{file}.arff\"\n",
    "    # Load arff file\n",
    "    data, meta = arff.loadarff(data_file)\n",
    "    data_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    #data_df = pd.read_csv('bitcoin_hist.csv')\n",
    "    #data_df[\"Date\"] =  pd.to_datetime(data_df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "    # data_df.plot(x='Date', y=['Close', 'Volume'], style=['r-', 'g-'], figsize=(10, 6))  # for stock\n",
    "\n",
    "    # # define the date format\n",
    "    # date_form = mdates.DateFormatter('%Y')\n",
    "\n",
    "    # # set the x-axis major locator to every even year\n",
    "    # ax = plt.gca()\n",
    "    # ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "    # ax.xaxis.set_major_formatter(date_form)\n",
    "    # ax.set_yscale(\"log\")\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    close_index = data_df.columns.get_loc('Close')\n",
    "    dataset = data_df.iloc[:, close_index:close_index+1].values  # numpy array\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "    # define early stopping\n",
    "    early_stopping5 = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "\n",
    "    class StopAtThreshold(Callback):\n",
    "        def __init__(self, monitor='loss', threshold=0.01):\n",
    "            super(StopAtThreshold, self).__init__()\n",
    "            self.monitor = monitor\n",
    "            self.threshold = threshold\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            current = logs.get(self.monitor)\n",
    "            if current is not None and current < self.threshold:\n",
    "                self.model.stop_training = True\n",
    "\n",
    "\n",
    "\n",
    "    # create an instance of our custom callback\n",
    "    stop_at_threshold = StopAtThreshold(monitor='val_loss', threshold=0.015)\n",
    "\n",
    "    def set_seeds(seed):\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        # If you are using CUDA, uncomment the following 2 lines\n",
    "        # os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "        # os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    set_seeds(1234)\n",
    "\n",
    "    # convert an array of values into a dataset matrix\n",
    "    def create_dataset(dataset, look_back=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-look_back):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    result = seasonal_test(data_df['Close'])\n",
    "    print(result)\n",
    "    trend, h, p, z, Tau, s, var_s, slope, intercept = result\n",
    "    import re\n",
    "    params=re.split(r'\\s*,\\s', \"trend, h, p, z, Tau, s, var_s, slope, intercept\")\n",
    "    for pr in params:\n",
    "        print(f'{pr}={eval(pr)}')\n",
    "\n",
    "\n",
    "\n",
    "    # because it's multiplicative, so apply np.log\n",
    "    dataset = np.log(dataset)\n",
    "\n",
    "    # Initialize a scaler for the dataset\n",
    "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Z-score normalization is useful when the data has outliers or when the distribution of the data is not known. \n",
    "    scaler = StandardScaler() \n",
    "\n",
    "    # Fit and transform the data to the scaler\n",
    "    # Split into train and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    training_ratio = training_ratio\n",
    "    train_data, test_data = train_test_split(dataset, train_size=training_ratio, shuffle=False)\n",
    "\n",
    "    # Fit the scaler to the training data and transform the training data\n",
    "    train = scaler.fit_transform(train_data)\n",
    "\n",
    "    # Use the same scaler to transform the test data\n",
    "    test = scaler.transform(test_data)\n",
    "    # print(train.shape, test.shape)### Using Multiple Layer Perceptron\n",
    "\n",
    "\n",
    "\n",
    "    # reshape dataset\n",
    "    look_back = look_back\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    test_data_with_look_back = np.concatenate((train[-look_back:], test))\n",
    "\n",
    "    # Create testing data, starting with the end of the training data\n",
    "    testX, testY = create_dataset(test_data_with_look_back, look_back)\n",
    "\n",
    "    def build_model(hp):\n",
    "       # Input layer   \n",
    "        model = Sequential()  \n",
    "        # model.add(Conv1D(filters=hp.Int('input_units',min_value=32,max_value=256,step=32)\n",
    "        #                 ,kernel_size=(look_back),activation='relu',input_shape=[look_back, 1]))\n",
    "        model.add(Conv1D(filters=160,kernel_size=(look_back),activation='relu',input_shape=[look_back, 1]))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "        # Output layer    \n",
    "        model.compile(optimizer=Adam(hp.Choice('learning_rate', [1e-1, 1e-2, 1e-3])),loss='mean_absolute_error')\n",
    "\n",
    "        modelname=f'CNN_{file}_{training_ratio}tr{look_back}lb{batch_size}bs_model.png'\n",
    "        tf.keras.utils.plot_model(model,show_shapes=True,show_layer_names=True, show_layer_activations=False,to_file=modelname)\n",
    "        return model\n",
    "\n",
    "    # create a TimeSeriesSplit object\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        executions_per_trial=3,\n",
    "        directory=directory,\n",
    "        project_name='bitcoin')\n",
    "\n",
    "    # define early stopping\n",
    "    early_stopping15 = EarlyStopping(monitor='val_loss', patience=15, verbose=0)\n",
    "    # create an instance of our custom callback\n",
    "    stop_at_threshold = StopAtThreshold(monitor='val_loss', threshold=0.01)\n",
    "    # perform hyperparameter tuning with time series cross-validation\n",
    "    for train_index, val_index in tscv.split(trainX):\n",
    "        X_train, X_val = trainX[train_index], trainX[val_index]\n",
    "        y_train, y_val = trainY[train_index], trainY[val_index]\n",
    "        tuner.search(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=10,\n",
    "            callbacks=[early_stopping15]\n",
    "            #callbacks=[stop_at_threshold]\n",
    "        )\n",
    "\n",
    "    # tuner.search_space_summary()\n",
    "    # get the best hyperparameters\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    # get the best trial\n",
    "    best_trial = tuner.oracle.get_best_trials()[0]\n",
    "\n",
    "\n",
    "\n",
    "    # get the score of the best trial\n",
    "    best_score = best_trial.score\n",
    "\n",
    "    # # print the score of the best trial\n",
    "    # print(f\"Best score: {best_score}\")\n",
    "\n",
    "    # print the values of the best hyperparameters\n",
    "    # for hp in best_hp.values:\n",
    "    #     print(f\"{hp}: {best_hp.get(hp)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # define early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=0)\n",
    "    # create an instance of our custom callback\n",
    "    stop_at_threshold = StopAtThreshold(monitor='val_loss', threshold=0.01)\n",
    "\n",
    "\n",
    "\n",
    "    ntrainX, valX, ntrainY, valY = train_test_split(trainX, trainY, test_size=0.1, shuffle=False)\n",
    "\n",
    "    #start_time = time.time()\n",
    "\n",
    "    # create a new HyperParameters object\n",
    "    new_hp = HyperParameters()\n",
    "\n",
    "    # set the hyperparameters to the desired values\n",
    "    new_hp.Fixed('input_units', 196)\n",
    "    new_hp.Fixed('learning_rate', 0.001)\n",
    "\n",
    "    # build a new model with the specified hyperparameters\n",
    "    model = build_model(new_hp)\n",
    "\n",
    "    # build the best model\n",
    "    # model = build_model(best_hp)\n",
    "\n",
    "    # fit the model with early stopping\n",
    "    history = model.fit(\n",
    "        ntrainX, ntrainY,\n",
    "        validation_data=(valX, valY),\n",
    "        epochs=1000,\n",
    "        batch_size=batch_size, \n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping15]\n",
    "        #callbacks=[stop_at_threshold]\n",
    "    )\n",
    "\n",
    "    # generate predictions for training\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # Inverse transform the predictions to original scale\n",
    "    trainPredict_orig = np.exp(scaler.inverse_transform(trainPredict))\n",
    "    trainY_orig = np.exp(scaler.inverse_transform([trainY]))\n",
    "    train_mse = mean_squared_error(trainY_orig[0], trainPredict_orig[:,0])\n",
    "    train_mae = mean_absolute_error(trainY_orig[0], trainPredict_orig[:,0])\n",
    "    \n",
    "\n",
    "    testPredict_orig = np.exp(scaler.inverse_transform(testPredict))\n",
    "    testY_orig = np.exp(scaler.inverse_transform([testY]))\n",
    "\n",
    "    # Now you can calculate your evaluation metrics on the original scale\n",
    "    test_mse = mean_squared_error(testY_orig[0], testPredict_orig[:,0])\n",
    "    test_mae = mean_absolute_error(testY_orig[0], testPredict_orig[:,0])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapse = end_time-start_time\n",
    "    elapse=f'{int(elapse//60)}m {int(elapse%60)}s'\n",
    "    # print(f'Total time: {elapse//60} minutes, {elapse%60:.4f} seconds.')\n",
    "\n",
    "    return best_score,best_hp.values['input_units'],best_hp.values['learning_rate'],elapse,math.sqrt(train_mse),train_mae,math.sqrt(test_mse),test_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 03s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 03s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.12499222904443741"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_loss: 0.12499222904443741"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val_loss So Far: 0.024088069175680477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best val_loss So Far: 0.024088069175680477"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 00h 00m 09s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 00h 00m 09s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'input_units'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bs \u001b[38;5;129;01min\u001b[39;00m batch_size:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataf \u001b[38;5;129;01min\u001b[39;00m data_file:\n\u001b[1;32m---> 34\u001b[0m         res\u001b[38;5;241m=\u001b[39m\u001b[43mCNN_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         data_file_list\u001b[38;5;241m.\u001b[39mappend(dataf)\n\u001b[0;32m     37\u001b[0m         training_ratio_list\u001b[38;5;241m.\u001b[39mappend(tr)\n",
      "Cell \u001b[1;32mIn[2], line 256\u001b[0m, in \u001b[0;36mCNN_train\u001b[1;34m(training_ratio, look_back, batch_size, file)\u001b[0m\n\u001b[0;32m    253\u001b[0m elapse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(elapse\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(elapse\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# print(f'Total time: {elapse//60} minutes, {elapse%60:.4f} seconds.')\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_score,\u001b[43mbest_hp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_units\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,best_hp\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],elapse,math\u001b[38;5;241m.\u001b[39msqrt(train_mse),train_mae,math\u001b[38;5;241m.\u001b[39msqrt(test_mse),test_mae\n",
      "\u001b[1;31mKeyError\u001b[0m: 'input_units'"
     ]
    }
   ],
   "source": [
    "# load a dataset\n",
    "# data_file=['BTCUSD-all','BTCUSD-N2Y','BTCUSD-N4Y',\n",
    "#            'ETHUSD-all','ETHUSD-N2Y','ETHUSD-N4Y',           \n",
    "#           'USDTUSD-all','USDTUSD-N2Y','USDTUSD-N4Y',  \n",
    "#            'BNBUSD-all','BNBUSD-N2Y','BNBUSD-N4Y']\n",
    "# data_file=['BTCUSD-1m1h','ETHUSD-1m1h','USDTUSD-1m1h','BNBUSD-1m1h']\n",
    "data_file=['BTCUSD-all']\n",
    "training_ratio=[0.7]\n",
    "look_back=[7]\n",
    "batch_size=[32]\n",
    "\n",
    "\n",
    "data_file_list= list()\n",
    "training_ratio_list= list()\n",
    "look_back_list= list()\n",
    "batch_size_list= list()\n",
    "\n",
    "best_score_list= list()\n",
    "input_units_list= list()\n",
    "learning_rate_list= list()\n",
    "elapse_list= list()\n",
    "\n",
    "train_rmse_list= list()\n",
    "train_mae_list= list()\n",
    "test_rmse_list= list()\n",
    "test_mae_list= list()\n",
    "\n",
    "\n",
    "\n",
    "for tr in training_ratio:\n",
    "    for lb in look_back:\n",
    "        for bs in batch_size:\n",
    "            for dataf in data_file:\n",
    "                res=CNN_train(tr,lb,bs,dataf)\n",
    "\n",
    "                data_file_list.append(dataf)\n",
    "                training_ratio_list.append(tr)\n",
    "                look_back_list.append(lb)\n",
    "                batch_size_list.append(bs)\n",
    "\n",
    "                best_score_list.append(res[0])\n",
    "                input_units_list.append(res[1])\n",
    "                learning_rate_list.append(res[2])\n",
    "                elapse_list.append(res[3])\n",
    "                \n",
    "                train_rmse_list.append(res[4])\n",
    "                train_mae_list.append(res[5])\n",
    "                test_rmse_list.append(res[6])\n",
    "                test_mae_list.append(res[7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>training ratio</th>\n",
       "      <th>look back</th>\n",
       "      <th>batch_size_list</th>\n",
       "      <th>best_score</th>\n",
       "      <th>input units</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>time train</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 54s</td>\n",
       "      <td>201.946225</td>\n",
       "      <td>480.794411</td>\n",
       "      <td>899.292595</td>\n",
       "      <td>1345.083957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 39s</td>\n",
       "      <td>226.476462</td>\n",
       "      <td>738.835559</td>\n",
       "      <td>2793.369867</td>\n",
       "      <td>3621.214934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 28s</td>\n",
       "      <td>277.049845</td>\n",
       "      <td>966.685962</td>\n",
       "      <td>4131.692223</td>\n",
       "      <td>5161.871618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.018247</td>\n",
       "      <td>256</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 25s</td>\n",
       "      <td>320.690455</td>\n",
       "      <td>1095.638766</td>\n",
       "      <td>4843.481472</td>\n",
       "      <td>5948.413515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.022984</td>\n",
       "      <td>160</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 41s</td>\n",
       "      <td>181.824693</td>\n",
       "      <td>485.313485</td>\n",
       "      <td>1108.642757</td>\n",
       "      <td>1606.302584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.022984</td>\n",
       "      <td>160</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 40s</td>\n",
       "      <td>177.079242</td>\n",
       "      <td>490.131885</td>\n",
       "      <td>987.433506</td>\n",
       "      <td>1473.167303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>0.022984</td>\n",
       "      <td>160</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 28s</td>\n",
       "      <td>214.062812</td>\n",
       "      <td>563.116748</td>\n",
       "      <td>1409.866728</td>\n",
       "      <td>1867.897735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>0.022984</td>\n",
       "      <td>160</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 32s</td>\n",
       "      <td>191.433417</td>\n",
       "      <td>522.843818</td>\n",
       "      <td>1278.900894</td>\n",
       "      <td>1732.781172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0.027540</td>\n",
       "      <td>224</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 51s</td>\n",
       "      <td>251.721520</td>\n",
       "      <td>599.900674</td>\n",
       "      <td>1892.322415</td>\n",
       "      <td>2397.576325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>0.027540</td>\n",
       "      <td>224</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 43s</td>\n",
       "      <td>205.495391</td>\n",
       "      <td>528.872780</td>\n",
       "      <td>1334.416628</td>\n",
       "      <td>1772.189672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>0.027540</td>\n",
       "      <td>224</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 36s</td>\n",
       "      <td>167.156295</td>\n",
       "      <td>465.247475</td>\n",
       "      <td>792.996860</td>\n",
       "      <td>1315.176264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>0.027540</td>\n",
       "      <td>224</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0m 34s</td>\n",
       "      <td>208.532391</td>\n",
       "      <td>533.864116</td>\n",
       "      <td>1286.148836</td>\n",
       "      <td>1713.718207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Data  training ratio  look back  batch_size_list  best_score  \\\n",
       "0   BTCUSD-all             0.7          1                4    0.018247   \n",
       "1   BTCUSD-all             0.7          1                8    0.018247   \n",
       "2   BTCUSD-all             0.7          1               16    0.018247   \n",
       "3   BTCUSD-all             0.7          1               32    0.018247   \n",
       "4   BTCUSD-all             0.7          7                4    0.022984   \n",
       "5   BTCUSD-all             0.7          7                8    0.022984   \n",
       "6   BTCUSD-all             0.7          7               16    0.022984   \n",
       "7   BTCUSD-all             0.7          7               32    0.022984   \n",
       "8   BTCUSD-all             0.7         14                4    0.027540   \n",
       "9   BTCUSD-all             0.7         14                8    0.027540   \n",
       "10  BTCUSD-all             0.7         14               16    0.027540   \n",
       "11  BTCUSD-all             0.7         14               32    0.027540   \n",
       "\n",
       "    input units  learning_rate time train   train_mae   train_rmse  \\\n",
       "0           256           0.01     0m 54s  201.946225   480.794411   \n",
       "1           256           0.01     0m 39s  226.476462   738.835559   \n",
       "2           256           0.01     0m 28s  277.049845   966.685962   \n",
       "3           256           0.01     0m 25s  320.690455  1095.638766   \n",
       "4           160           0.01     0m 41s  181.824693   485.313485   \n",
       "5           160           0.01     0m 40s  177.079242   490.131885   \n",
       "6           160           0.01     0m 28s  214.062812   563.116748   \n",
       "7           160           0.01     0m 32s  191.433417   522.843818   \n",
       "8           224           0.01     0m 51s  251.721520   599.900674   \n",
       "9           224           0.01     0m 43s  205.495391   528.872780   \n",
       "10          224           0.01     0m 36s  167.156295   465.247475   \n",
       "11          224           0.01     0m 34s  208.532391   533.864116   \n",
       "\n",
       "       test_mae    test_rmse  \n",
       "0    899.292595  1345.083957  \n",
       "1   2793.369867  3621.214934  \n",
       "2   4131.692223  5161.871618  \n",
       "3   4843.481472  5948.413515  \n",
       "4   1108.642757  1606.302584  \n",
       "5    987.433506  1473.167303  \n",
       "6   1409.866728  1867.897735  \n",
       "7   1278.900894  1732.781172  \n",
       "8   1892.322415  2397.576325  \n",
       "9   1334.416628  1772.189672  \n",
       "10   792.996860  1315.176264  \n",
       "11  1286.148836  1713.718207  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Summary={'Data':data_file_list,'training ratio':training_ratio_list,'look back':look_back_list,'batch_size_list':batch_size_list,\n",
    "         'best_score':best_score_list,'input units':input_units_list,'learning_rate':learning_rate_list,'time train':elapse_list,\n",
    "            'train_mae':train_mae_list,          \n",
    "            'train_rmse':train_rmse_list,          \n",
    "            'test_mae':test_mae_list,           \n",
    "            'test_rmse':test_rmse_list\n",
    "            \n",
    "         }\n",
    "\n",
    "df_Summary = pd.DataFrame(Summary)\n",
    "# df_Summary.to_excel(\"Summary-BiLSTM(1m1g).xlsx\",index=False)  \n",
    "df_Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
