{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.layers import Conv1D, LSTM, Lambda, Dropout,Bidirectional\n",
    "from keras_tuner import RandomSearch\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pymannkendall import seasonal_test\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import re\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import HyperParameters\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp,look_back):\n",
    "    model = Sequential()\n",
    "    # Input layer   \n",
    "    # add a Bidirectional LSTM layer    \n",
    "    model.add(Bidirectional(LSTM(units=hp.Int('input_units',min_value=32,max_value=256,step=32),\n",
    "                                  return_sequences=True),input_shape=(look_back, 1)))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units=hp.Int('input_units',min_value=32,max_value=256,step=32)),input_shape=(look_back, 1)))\n",
    "    \n",
    "   \n",
    "    # Output layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', [1e-1, 1e-2, 1e-3])),loss='mean_absolute_error')\n",
    "    \n",
    "    tf.keras.utils.plot_model(model,show_shapes=True,show_layer_names=True, show_layer_activations=False,to_file='BiLSTM_model.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopAtThreshold(Callback):\n",
    "    def __init__(self, monitor='loss', threshold=0.01):\n",
    "        super(StopAtThreshold, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is not None and current < self.threshold:\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    # If you are using CUDA, uncomment the following 2 lines\n",
    "    # os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    # os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMTrain(data_file,training_ratio,look_back,batch_size):\n",
    "    \n",
    "    # Load arff file\n",
    "    data, meta = arff.loadarff((f'../Data/WekaData/{data_file}.arff'))\n",
    "    data_df = pd.DataFrame(data)\n",
    "    \n",
    "    close_index = data_df.columns.get_loc('Close')\n",
    "    dataset = data_df.iloc[:, close_index:close_index+1].values  # numpy array\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "\n",
    "    # define early stopping\n",
    "    early_stopping5 = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "\n",
    "\n",
    "    # create an instance of our custom callback\n",
    "    stop_at_threshold = StopAtThreshold(monitor='val_loss', threshold=0.015)\n",
    "    \n",
    "\n",
    "    set_seeds(1234)  \n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    # because it's multiplicative, so apply np.log\n",
    "    dataset = np.log(dataset)\n",
    "\n",
    "    # Initialize a scaler for the dataset\n",
    "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Z-score normalization is useful when the data has outliers or when the distribution of the data is not known. \n",
    "    scaler = StandardScaler() \n",
    "\n",
    "    # Fit and transform the data to the scaler\n",
    "    # Split into train and test sets\n",
    "\n",
    "    training_ratio = training_ratio\n",
    "    train_data, test_data = train_test_split(dataset, train_size=training_ratio, shuffle=False)\n",
    "\n",
    "    # Fit the scaler to the training data and transform the training data\n",
    "    train = scaler.fit_transform(train_data)\n",
    "\n",
    "    # Use the same scaler to transform the test data\n",
    "    test = scaler.transform(test_data)\n",
    "    print(train.shape, test.shape)### Using Multiple Layer Perceptron\n",
    "\n",
    "\n",
    "\n",
    "    # reshape dataset\n",
    "    look_back = look_back\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    test_data_with_look_back = np.concatenate((train[-look_back:], test))\n",
    "\n",
    "    # Create testing data, starting with the end of the training data\n",
    "    testX, testY = create_dataset(test_data_with_look_back, look_back)\n",
    "\n",
    "    # create a TimeSeriesSplit object\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        executions_per_trial=3,\n",
    "        project_name='bitcoin')\n",
    "\n",
    "    # define early stopping\n",
    "    early_stopping15 = EarlyStopping(monitor='val_loss', patience=15, verbose=0)\n",
    "    # create an instance of our custom callback\n",
    "    stop_at_threshold = StopAtThreshold(monitor='val_loss', threshold=0.01)\n",
    "    # perform hyperparameter tuning with time series cross-validation\n",
    "    for train_index, val_index in tscv.split(trainX):\n",
    "        X_train, X_val = trainX[train_index], trainX[val_index]\n",
    "        y_train, y_val = trainY[train_index], trainY[val_index]\n",
    "        tuner.search(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=10,\n",
    "            callbacks=[early_stopping15]\n",
    "            #callbacks=[stop_at_threshold]\n",
    "        )\n",
    "\n",
    "    # tuner.search_space_summary()\n",
    "    # get the best hyperparameters\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    # get the best trial\n",
    "    best_trial = tuner.oracle.get_best_trials()[0]\n",
    "\n",
    "\n",
    "\n",
    "    # get the score of the best trial\n",
    "    best_score = best_trial.score\n",
    "\n",
    "    # print the score of the best trial\n",
    "    print(f\"Best score: {best_score}\")\n",
    "\n",
    "    # print the values of the best hyperparameters\n",
    "    for hp in best_hp.values:\n",
    "        print(f\"{hp}: {best_hp.get(hp)}\")\n",
    "\n",
    "\n",
    "    # define early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=0)\n",
    "    # create an instance of our custom callback\n",
    "    stop_at_threshold = StopAtThreshold(monitor='val_loss', threshold=0.01)\n",
    "\n",
    "\n",
    "\n",
    "    ntrainX, valX, ntrainY, valY = train_test_split(trainX, trainY, test_size=0.1, shuffle=False)\n",
    "\n",
    "    #start_time = time.time()\n",
    "\n",
    "    # create a new HyperParameters object\n",
    "    new_hp = HyperParameters()\n",
    "\n",
    "    # set the hyperparameters to the desired values\n",
    "    # new_hp.Fixed('input_units', 228)\n",
    "    # new_hp.Fixed('learning_rate', 0.01)\n",
    "    new_hp.Fixed('input_units', best_hp.values['input_units'])\n",
    "    new_hp.Fixed('learning_rate', best_hp.values['learning_rate'])\n",
    "\n",
    "    # build a new model with the specified hyperparameters\n",
    "    model = build_model(new_hp,look_back)\n",
    "\n",
    "    # build the best model\n",
    "    # model = build_model(best_hp)\n",
    "\n",
    "    # fit the model with early stopping\n",
    "    history = model.fit(\n",
    "        ntrainX, ntrainY,\n",
    "        validation_data=(valX, valY),\n",
    "        epochs=1000,\n",
    "        batch_size=batch_size, \n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping15]\n",
    "        #callbacks=[stop_at_threshold]\n",
    "    )\n",
    "\n",
    "    # generate predictions for training\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # Inverse transform the predictions to original scale\n",
    "    trainPredict_orig = np.exp(scaler.inverse_transform(trainPredict))\n",
    "    trainY_orig = np.exp(scaler.inverse_transform([trainY]))\n",
    "    train_mse = mean_squared_error(trainY_orig[0], trainPredict_orig[:,0])\n",
    "    train_mae = mean_absolute_error(trainY_orig[0], trainPredict_orig[:,0])\n",
    "    print(f'train MSE: {train_mse:.4f}, RMSE: {math.sqrt(train_mse):.4f}, MAE: {train_mae:.4f}')\n",
    "\n",
    "    testPredict_orig = np.exp(scaler.inverse_transform(testPredict))\n",
    "    testY_orig = np.exp(scaler.inverse_transform([testY]))\n",
    "\n",
    "    # Now you can calculate your evaluation metrics on the original scale\n",
    "    test_mse = mean_squared_error(testY_orig[0], testPredict_orig[:,0])\n",
    "    test_mae = mean_absolute_error(testY_orig[0], testPredict_orig[:,0])\n",
    "    print(f'test MSE: {test_mse:.4f}, RMSE: {math.sqrt(test_mse):.4f}, MAE: {test_mae:.4f}')\n",
    "  \n",
    "   \n",
    "    return best_score,best_hp.values['input_units'],best_hp.values['learning_rate'],math.sqrt(train_mse),train_mae,math.sqrt(test_mse),test_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(503, 1) (217, 1)\n",
      "INFO:tensorflow:Reloading Tuner from .\\bitcoin\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Best score: 0.037774731094638504\n",
      "input_units: 228\n",
      "learning_rate: 0.01\n",
      "16/16 [==============================] - 1s 16ms/step\n",
      "7/7 [==============================] - 0s 19ms/step\n",
      "train MSE: 26035.0992, RMSE: 161.3540, MAE: 107.9584\n",
      "test MSE: 20112.5171, RMSE: 141.8186, MAE: 103.7820\n"
     ]
    }
   ],
   "source": [
    "# load a dataset\n",
    "data_file=['BTCUSD-1m1h']\n",
    "\n",
    "\n",
    "training_ratio=[0.7]\n",
    "look_backs=[7]\n",
    "batch_size=[32]\n",
    "\n",
    "# data_file=['BTCUSD-all']\n",
    "# training_ratio=[0.7]\n",
    "# look_backs=[1]\n",
    "# batch_size=[4]\n",
    "\n",
    "\n",
    "data_file_list= list()\n",
    "training_ratio_list= list()\n",
    "look_back_list= list()\n",
    "batch_size_list= list()\n",
    "\n",
    "best_score_list= list()\n",
    "input_units_list= list()\n",
    "learning_rate_list= list()\n",
    "\n",
    "train_rmse_list= list()\n",
    "train_mae_list= list()\n",
    "test_rmse_list= list()\n",
    "test_mae_list= list()\n",
    "\n",
    "for df in data_file:\n",
    "    for tr in training_ratio:\n",
    "        for lb in look_backs:\n",
    "            for bs in batch_size:\n",
    "                data_file_list.append(df)\n",
    "                training_ratio_list.append(tr)\n",
    "                look_back_list.append(lb)\n",
    "                batch_size_list.append(bs)\n",
    "                resultLSTM=LSTMTrain(df,tr,lb,bs)\n",
    "                best_score_list.append(resultLSTM[0])\n",
    "                input_units_list.append(resultLSTM[1])\n",
    "                learning_rate_list.append(resultLSTM[2])\n",
    "                train_rmse_list.append(resultLSTM[3])\n",
    "                train_mae_list.append(resultLSTM[4])\n",
    "                test_rmse_list.append(resultLSTM[5])\n",
    "                test_mae_list.append(resultLSTM[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>training ratio</th>\n",
       "      <th>look back</th>\n",
       "      <th>batch_size_list</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>340.017702</td>\n",
       "      <td>1211.01567</td>\n",
       "      <td>9223.188972</td>\n",
       "      <td>11586.226742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Data  training ratio  look back  batch_size_list   train_mae  \\\n",
       "0  BTCUSD-all             0.7          1                4  340.017702   \n",
       "\n",
       "   train_rmse     test_mae     test_rmse  \n",
       "0  1211.01567  9223.188972  11586.226742  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summary={'Data':data_file_list,'training ratio':training_ratio_list,'look back':look_back_list,\n",
    "         'batch_size_list':batch_size_list,\n",
    "            'train_mae':train_mae_list,          \n",
    "            'train_rmse':train_rmse_list,          \n",
    "            'test_mae':test_mae_list,           \n",
    "            'test_rmse':test_rmse_list\n",
    "            \n",
    "         }\n",
    "\n",
    "df_Summary = pd.DataFrame(Summary)\n",
    "# df_Summary.to_excel(\"Summary-BiLSTM-1h1Y.xlsx\",index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
