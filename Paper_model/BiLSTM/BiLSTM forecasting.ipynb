{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import sys\n",
    "from tensorflow.keras.utils import disable_interactive_logging\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pymannkendall import seasonal_test\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv1D, LSTM, Lambda, Dropout,Bidirectional\n",
    "from keras_tuner import RandomSearch\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_tuner import HyperParameters\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_train(training_ratio,look_back,batch_size,file):\n",
    "    disable_interactive_logging()\n",
    "    class Tee:\n",
    "        def __init__(self, *files):\n",
    "            self.files = files\n",
    "        def write(self, obj):\n",
    "            for f in self.files:\n",
    "                f.write(obj)\n",
    "                f.flush()\n",
    "        def flush(self):\n",
    "            for f in self.files:\n",
    "                f.flush()\n",
    "\n",
    "    sys.stdout = Tee(sys.stdout, sys.stderr)\n",
    "\n",
    "    directory=f'lstm_{training_ratio}_{look_back}_{batch_size}'\n",
    "    if os.path.exists(directory):    \n",
    "        shutil.rmtree(directory)\n",
    "        print(f\"{directory} is removed successfully\")\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # load a dataset\n",
    "    data_file = f\"../Data/WekaData/{file}.arff\"\n",
    "    # Load arff file\n",
    "    data, meta = arff.loadarff(data_file)\n",
    "    data_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "    # Convert to pandas DataFrame\n",
    "    #data_df = pd.read_csv('bitcoin_hist.csv')\n",
    "    #data_df[\"Date\"] =  pd.to_datetime(data_df[\"Date\"], format=\"%m/%d/%Y\")\n",
    "\n",
    "    # data_df.plot(x='Date', y=['Close', 'Volume'], style=['r-', 'g-'], figsize=(10, 6))  # for stock\n",
    "\n",
    "    # # define the date format\n",
    "    # date_form = mdates.DateFormatter('%Y')\n",
    "\n",
    "    # # set the x-axis major locator to every even year\n",
    "    # ax = plt.gca()\n",
    "    # ax.xaxis.set_major_locator(mdates.YearLocator(base=1))\n",
    "    # ax.xaxis.set_major_formatter(date_form)\n",
    "    # ax.set_yscale(\"log\")\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    close_index = data_df.columns.get_loc('Close')\n",
    "    dataset = data_df.iloc[:, close_index:close_index+1].values  # numpy array\n",
    "    dataset = dataset.astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "    # define early stopping\n",
    "    early_stopping5 = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n",
    "\n",
    "    class StopAtThreshold(Callback):\n",
    "        def __init__(self, monitor='loss', threshold=0.01):\n",
    "            super(StopAtThreshold, self).__init__()\n",
    "            self.monitor = monitor\n",
    "            self.threshold = threshold\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            current = logs.get(self.monitor)\n",
    "            if current is not None and current < self.threshold:\n",
    "                self.model.stop_training = True\n",
    "\n",
    "\n",
    "\n",
    "    # create an instance of our custom callback\n",
    "    stop_at_threshold = StopAtThreshold(monitor='val_loss', threshold=0.015)\n",
    "\n",
    "    def set_seeds(seed):\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        # If you are using CUDA, uncomment the following 2 lines\n",
    "        # os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "        # os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    set_seeds(1234)\n",
    "\n",
    "    # convert an array of values into a dataset matrix\n",
    "    def create_dataset(dataset, look_back=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-look_back):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    result = seasonal_test(data_df['Close'])\n",
    "    print(result)\n",
    "    trend, h, p, z, Tau, s, var_s, slope, intercept = result\n",
    "    import re\n",
    "    params=re.split(r'\\s*,\\s', \"trend, h, p, z, Tau, s, var_s, slope, intercept\")\n",
    "    for pr in params:\n",
    "        print(f'{pr}={eval(pr)}')\n",
    "\n",
    "\n",
    "\n",
    "    # because it's multiplicative, so apply np.log\n",
    "    dataset = np.log(dataset)\n",
    "\n",
    "    # Initialize a scaler for the dataset\n",
    "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # Z-score normalization is useful when the data has outliers or when the distribution of the data is not known. \n",
    "    scaler = StandardScaler() \n",
    "\n",
    "    # Fit and transform the data to the scaler\n",
    "    # Split into train and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    training_ratio = training_ratio\n",
    "    train_data, test_data = train_test_split(dataset, train_size=training_ratio, shuffle=False)\n",
    "\n",
    "    # Fit the scaler to the training data and transform the training data\n",
    "    train = scaler.fit_transform(train_data)\n",
    "\n",
    "    # Use the same scaler to transform the test data\n",
    "    test = scaler.transform(test_data)\n",
    "    # print(train.shape, test.shape)### Using Multiple Layer Perceptron\n",
    "\n",
    "\n",
    "\n",
    "    # reshape dataset\n",
    "    look_back = look_back\n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    test_data_with_look_back = np.concatenate((train[-look_back:], test))\n",
    "\n",
    "    # Create testing data, starting with the end of the training data\n",
    "    testX, testY = create_dataset(test_data_with_look_back, look_back)\n",
    "\n",
    "    def build_model(hp):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Input layer   \n",
    "        # add a Bidirectional LSTM layer    \n",
    "        # model.add(Bidirectional(LSTM(units=hp.Int('input_units',min_value=100,max_value=256,step=16),\n",
    "        #                               return_sequences=True),input_shape=(look_back, 1)))\n",
    "        # model.add(Bidirectional(LSTM(units=hp.Int('input_units',min_value=100,max_value=256,step=16)),input_shape=(look_back, 1)))\n",
    "\n",
    "        model.add(Bidirectional(LSTM(units=212,return_sequences=True),input_shape=(look_back, 1)))\n",
    "        model.add(Bidirectional(LSTM(units=212,input_shape=(look_back, 1))))\n",
    "        \n",
    "        # model.add(Bidirectional(LSTM(units=hp.Int('input_units',min_value=100,max_value=256,step=16)),input_shape=(look_back, 1)))\n",
    "    \n",
    "        # Output layer\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer=Adam(hp.Choice('learning_rate', [1e-1, 1e-2, 1e-3])),loss='mean_absolute_error')\n",
    "\n",
    "        modelname=f'BiLSTM_{file}_{training_ratio}tr{look_back}lb{batch_size}bs_model.png'\n",
    "        # tf.keras.utils.plot_model(model,show_shapes=True,show_layer_names=True, show_layer_activations=False,to_file=modelname)\n",
    "        tf.keras.utils.plot_model(model,show_shapes=True,show_layer_names=True,rankdir='TB',to_file=modelname,dpi=900,expand_nested=True)\n",
    "        return model\n",
    "\n",
    "    # create a TimeSeriesSplit object\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        executions_per_trial=3,\n",
    "        directory=directory,\n",
    "        project_name='bitcoin')\n",
    "\n",
    "    # define early stopping\n",
    "    early_stopping15 = EarlyStopping(monitor='val_loss', patience=15, verbose=0)\n",
    "    # create an instance of our custom callback\n",
    "    stop_at_threshold = StopAtThreshold(monitor='val_loss', threshold=0.01)\n",
    "    # perform hyperparameter tuning with time series cross-validation\n",
    "    for train_index, val_index in tscv.split(trainX):\n",
    "        X_train, X_val = trainX[train_index], trainX[val_index]\n",
    "        y_train, y_val = trainY[train_index], trainY[val_index]\n",
    "        tuner.search(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=10,\n",
    "            callbacks=[early_stopping15]\n",
    "            #callbacks=[stop_at_threshold]\n",
    "        )\n",
    "\n",
    "    # tuner.search_space_summary()\n",
    "    # get the best hyperparameters\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    # get the best trial\n",
    "    best_trial = tuner.oracle.get_best_trials()[0]\n",
    "\n",
    "\n",
    "\n",
    "    # get the score of the best trial\n",
    "    best_score = best_trial.score\n",
    "\n",
    "    # # print the score of the best trial\n",
    "    # print(f\"Best score: {best_score}\")\n",
    "\n",
    "    # print the values of the best hyperparameters\n",
    "    # for hp in best_hp.values:\n",
    "    #     print(f\"{hp}: {best_hp.get(hp)}\")\n",
    "\n",
    "\n",
    "\n",
    "    # define early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=0)\n",
    "    # create an instance of our custom callback\n",
    "    stop_at_threshold = StopAtThreshold(monitor='val_loss', threshold=0.01)\n",
    "\n",
    "\n",
    "\n",
    "    ntrainX, valX, ntrainY, valY = train_test_split(trainX, trainY, test_size=0.1, shuffle=False)\n",
    "\n",
    "    #start_time = time.time()\n",
    "\n",
    "    # create a new HyperParameters object\n",
    "    new_hp = HyperParameters()\n",
    "\n",
    "    # set the hyperparameters to the desired values\n",
    "    new_hp.Fixed('input_units',196)\n",
    "    new_hp.Fixed('learning_rate',0.001)\n",
    "\n",
    "    # build a new model with the specified hyperparameters\n",
    "    model = build_model(new_hp)\n",
    "\n",
    "    # build the best model\n",
    "    # model = build_model(best_hp)\n",
    "\n",
    "    # fit the model with early stopping\n",
    "    history = model.fit(\n",
    "        ntrainX, ntrainY,\n",
    "        validation_data=(valX, valY),\n",
    "        epochs=1000,\n",
    "        batch_size=batch_size, \n",
    "        verbose=0,\n",
    "        callbacks=[early_stopping15]\n",
    "        #callbacks=[stop_at_threshold]\n",
    "    )\n",
    "\n",
    "    # generate predictions for training\n",
    "    trainPredict = model.predict(trainX)\n",
    "    testPredict = model.predict(testX)\n",
    "\n",
    "    # Inverse transform the predictions to original scale\n",
    "    trainPredict_orig = np.exp(scaler.inverse_transform(trainPredict))\n",
    "    trainY_orig = np.exp(scaler.inverse_transform([trainY]))\n",
    "    train_mse = mean_squared_error(trainY_orig[0], trainPredict_orig[:,0])\n",
    "    train_mae = mean_absolute_error(trainY_orig[0], trainPredict_orig[:,0])\n",
    "    \n",
    "\n",
    "    testPredict_orig = np.exp(scaler.inverse_transform(testPredict))\n",
    "    testY_orig = np.exp(scaler.inverse_transform([testY]))\n",
    "\n",
    "    # Now you can calculate your evaluation metrics on the original scale\n",
    "    test_mse = mean_squared_error(testY_orig[0], testPredict_orig[:,0])\n",
    "    test_mae = mean_absolute_error(testY_orig[0], testPredict_orig[:,0])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapse = end_time-start_time\n",
    "    elapse=f'{int(elapse//60)}m {int(elapse%60)}s'\n",
    "    # print(f'Total time: {elapse//60} minutes, {elapse%60:.4f} seconds.')\n",
    "\n",
    "    return best_score,best_hp.values['input_units'],best_hp.values['learning_rate'],elapse,math.sqrt(train_mse),train_mae,math.sqrt(test_mse),test_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 50s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 3 Complete [00h 00m 50s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.2085264523824056"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_loss: 0.2085264523824056"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val_loss So Far: 0.023964376499255497"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best val_loss So Far: 0.023964376499255497"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 00h 02m 14s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total elapsed time: 00h 02m 14s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'input_units'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bs \u001b[38;5;129;01min\u001b[39;00m batch_size:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dataf \u001b[38;5;129;01min\u001b[39;00m data_file:\n\u001b[1;32m---> 34\u001b[0m         res\u001b[38;5;241m=\u001b[39m\u001b[43mLSTM_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m         data_file_list\u001b[38;5;241m.\u001b[39mappend(dataf)\n\u001b[0;32m     37\u001b[0m         training_ratio_list\u001b[38;5;241m.\u001b[39mappend(tr)\n",
      "Cell \u001b[1;32mIn[2], line 265\u001b[0m, in \u001b[0;36mLSTM_train\u001b[1;34m(training_ratio, look_back, batch_size, file)\u001b[0m\n\u001b[0;32m    262\u001b[0m elapse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(elapse\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(elapse\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# print(f'Total time: {elapse//60} minutes, {elapse%60:.4f} seconds.')\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_score,\u001b[43mbest_hp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_units\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,best_hp\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],elapse,math\u001b[38;5;241m.\u001b[39msqrt(train_mse),train_mae,math\u001b[38;5;241m.\u001b[39msqrt(test_mse),test_mae\n",
      "\u001b[1;31mKeyError\u001b[0m: 'input_units'"
     ]
    }
   ],
   "source": [
    "# load a dataset\n",
    "# data_file=['BTCUSD-all','BTCUSD-N2Y','BTCUSD-N4Y',\n",
    "#            'ETHUSD-all','ETHUSD-N2Y','ETHUSD-N4Y',           \n",
    "#           'USDTUSD-all','USDTUSD-N2Y','USDTUSD-N4Y',  \n",
    "#            'BNBUSD-all','BNBUSD-N2Y','BNBUSD-N4Y']\n",
    "# data_file=['BTCUSD-1m1h','ETHUSD-1m1h','USDTUSD-1m1h','BNBUSD-1m1h']\n",
    "data_file=['BTCUSD-all']\n",
    "training_ratio=[0.7]\n",
    "look_back=[7]\n",
    "batch_size=[16]\n",
    "\n",
    "\n",
    "data_file_list= list()\n",
    "training_ratio_list= list()\n",
    "look_back_list= list()\n",
    "batch_size_list= list()\n",
    "\n",
    "best_score_list= list()\n",
    "input_units_list= list()\n",
    "learning_rate_list= list()\n",
    "elapse_list= list()\n",
    "\n",
    "train_rmse_list= list()\n",
    "train_mae_list= list()\n",
    "test_rmse_list= list()\n",
    "test_mae_list= list()\n",
    "\n",
    "\n",
    "\n",
    "for tr in training_ratio:\n",
    "    for lb in look_back:\n",
    "        for bs in batch_size:\n",
    "            for dataf in data_file:\n",
    "                res=LSTM_train(tr,lb,bs,dataf)\n",
    "\n",
    "                data_file_list.append(dataf)\n",
    "                training_ratio_list.append(tr)\n",
    "                look_back_list.append(lb)\n",
    "                batch_size_list.append(bs)\n",
    "\n",
    "                best_score_list.append(res[0])\n",
    "                input_units_list.append(res[1])\n",
    "                learning_rate_list.append(res[2])\n",
    "                elapse_list.append(res[3])\n",
    "                \n",
    "                train_rmse_list.append(res[4])\n",
    "                train_mae_list.append(res[5])\n",
    "                test_rmse_list.append(res[6])\n",
    "                test_mae_list.append(res[7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>training ratio</th>\n",
       "      <th>look back</th>\n",
       "      <th>batch_size_list</th>\n",
       "      <th>best_score</th>\n",
       "      <th>input units</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>time train</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>train_rmse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BTCUSD-all</td>\n",
       "      <td>0.7</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>0.053836</td>\n",
       "      <td>212</td>\n",
       "      <td>0.01</td>\n",
       "      <td>11m 11s</td>\n",
       "      <td>230.490532</td>\n",
       "      <td>553.219972</td>\n",
       "      <td>1701.320124</td>\n",
       "      <td>2170.920935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Data  training ratio  look back  batch_size_list  best_score  \\\n",
       "0  BTCUSD-all             0.7          7               16    0.053836   \n",
       "\n",
       "   input units  learning_rate time train   train_mae  train_rmse     test_mae  \\\n",
       "0          212           0.01    11m 11s  230.490532  553.219972  1701.320124   \n",
       "\n",
       "     test_rmse  \n",
       "0  2170.920935  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Summary={'Data':data_file_list,'training ratio':training_ratio_list,'look back':look_back_list,'batch_size_list':batch_size_list,\n",
    "         'best_score':best_score_list,'input units':input_units_list,'learning_rate':learning_rate_list,'time train':elapse_list,\n",
    "            'train_mae':train_mae_list,          \n",
    "            'train_rmse':train_rmse_list,          \n",
    "            'test_mae':test_mae_list,           \n",
    "            'test_rmse':test_rmse_list\n",
    "            \n",
    "         }\n",
    "\n",
    "df_Summary = pd.DataFrame(Summary)\n",
    "# df_Summary.to_excel(\"Summary-BiLSTM(1m1g).xlsx\",index=False)  \n",
    "df_Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
