{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-table:\n",
      "[[[  85.46285278   32.30944661   97.0299       84.07688756]\n",
      "  [   0.79857279 -100.         -100.           64.57577892]\n",
      "  [   0.            0.            0.            0.        ]]\n",
      "\n",
      " [[  79.16595172 -100.           98.01         69.53096376]\n",
      "  [   0.            0.            0.            0.        ]\n",
      "  [   0.            0.            0.            0.        ]]\n",
      "\n",
      " [[  87.0454805    99.           94.50159517   92.35883193]\n",
      "  [-100.          100.           86.9639112    87.1404317 ]\n",
      "  [   0.            0.            0.            0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定義簡單的格子世界環境\n",
    "# S：起始點\n",
    "# F：一般方塊\n",
    "# G：目標方塊\n",
    "# H：陷阱方塊\n",
    "# 可行動作：上（0）、右（1）、下（2）、左（3）\n",
    "grid_world = [\n",
    "    ['S', 'F', 'H'],\n",
    "    ['F', 'H', 'F'],\n",
    "    ['F', 'F', 'G']\n",
    "]\n",
    "\n",
    "# 設置環境參數\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01\n",
    "\n",
    "# 取得環境尺寸\n",
    "num_rows = len(grid_world)\n",
    "num_cols = len(grid_world[0])\n",
    "num_actions = 4  # 上、右、下、左\n",
    "\n",
    "# 建立 Q-table（狀態-動作價值表）\n",
    "q_table = np.zeros((num_rows, num_cols, num_actions))\n",
    "\n",
    "# Q-learning 算法\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)  # 起始狀態\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # 使用 ε-貪心策略選擇行動\n",
    "        exploration_threshold = np.random.uniform(0, 1)\n",
    "        if exploration_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state])\n",
    "        else:\n",
    "            action = np.random.randint(0, num_actions)\n",
    "\n",
    "        # 執行選擇的行動並觀察新狀態和獎勵\n",
    "        new_state = state\n",
    "        if action == 0 and state[0] > 0:  # 上\n",
    "            new_state = (state[0] - 1, state[1])\n",
    "        elif action == 1 and state[1] < num_cols - 1:  # 右\n",
    "            new_state = (state[0], state[1] + 1)\n",
    "        elif action == 2 and state[0] < num_rows - 1:  # 下\n",
    "            new_state = (state[0] + 1, state[1])\n",
    "        elif action == 3 and state[1] > 0:  # 左\n",
    "            new_state = (state[0], state[1] - 1)\n",
    "\n",
    "        # 更新 Q-table\n",
    "        if grid_world[new_state[0]][new_state[1]] == 'G':  # 到達目標\n",
    "            q_table[state][action] = 100\n",
    "            break\n",
    "        elif grid_world[new_state[0]][new_state[1]] == 'H':  # 陷阱\n",
    "            q_table[state][action] = -100\n",
    "            break\n",
    "        else:\n",
    "            # Q-learning 更新公式\n",
    "            q_table[state][action] = q_table[state][action] + learning_rate * (\n",
    "                0 + discount_rate * np.max(q_table[new_state]) - q_table[state][action]\n",
    "            )\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    # 降低探索率\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "\n",
    "# 印出最終的 Q-table\n",
    "print(\"Final Q-table:\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResetNeeded",
     "evalue": "Cannot call `env.render()` before calling `env.reset()`, if this is a intended action, set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResetNeeded\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nptu\\Documents\\GitHub\\Market-Sentiment-Analysis\\AI model\\CH11\\Q-learning.ipynb 儲存格 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nptu/Documents/GitHub/Market-Sentiment-Analysis/AI%20model/CH11/Q-learning.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtime\u001b[39;00m \u001b[39mimport\u001b[39;00m sleep\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nptu/Documents/GitHub/Market-Sentiment-Analysis/AI%20model/CH11/Q-learning.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mTaxi-v3\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m#创建出租车游戏环境\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nptu/Documents/GitHub/Market-Sentiment-Analysis/AI%20model/CH11/Q-learning.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender() \u001b[39m# 用于渲染出当前的智能体以及环境的状态\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nptu/Documents/GitHub/Market-Sentiment-Analysis/AI%20model/CH11/Q-learning.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# 将Q表初始化为一个字典，它存储指定在状态s中执行动作a的值的状态-动作对。\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nptu/Documents/GitHub/Market-Sentiment-Analysis/AI%20model/CH11/Q-learning.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m q \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\nptu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\n\u001b[0;32m    326\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    327\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[0;32m    328\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nptu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:47\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Renders the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m     )\n\u001b[0;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mResetNeeded\u001b[0m: Cannot call `env.render()` before calling `env.reset()`, if this is a intended action, set `disable_render_order_enforcing=True` on the OrderEnforcer wrapper."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "from time import sleep\n",
    " \n",
    "env = gym.make('Taxi-v3') #创建出租车游戏环境\n",
    "env.render() # 用于渲染出当前的智能体以及环境的状态\n",
    " \n",
    "# 将Q表初始化为一个字典，它存储指定在状态s中执行动作a的值的状态-动作对。\n",
    "q = {}\n",
    "for s in range(env.observation_space.n):\n",
    "    for a in range(env.action_space.n):\n",
    "        q[(s,a)] = 0.0\n",
    " \n",
    "# 定义一个名为update_q_table的函数，根据q学习更新规则更新q值\n",
    "def update_q_table(prev_state, action, reward, nextstate, alpha, gamma):\n",
    "    qa = max([q[(nextstate, a)] for a in range(env.action_space.n)]) # 取一个状态-动作对的最大值，并将其存储在一个名为qa的变量中\n",
    "    # max Q(s', a')\n",
    "    q[(prev_state, action)] += alpha * (reward + gamma * qa - q[(prev_state, action)]) # 用更新规则更新前一个状态的Q值\n",
    "    # Q(s, a) <- Q(s, a) + alpha (r + gamma max Q(s', a') - Q(s, a))\n",
    " \n",
    "# epsilon贪心策略函数\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample() # 随机，用epsilon概率探索新动作\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)), key = lambda x: q[(state,x)]) # 用1-epsilon的概率选择Q表最佳动作\n",
    " \n",
    "# 初始化变量\n",
    "alpha = 0.4  # TD学习率\n",
    "gamma = 0.999 # 折扣率\n",
    "epsilon = 0.017 # 贪心策略中epsilon的值\n",
    "num_episodes = 1000 # 玩几局游戏\n",
    " \n",
    "# 执行Q-Learning\n",
    "for episode in range(num_episodes): # 玩几局游戏\n",
    "    steps, r = 0, 0 # 每局走多少步，总体奖励\n",
    "    prev_state = env.reset() # 用于重置环境\n",
    "    while True:\n",
    "        steps += 1 # 每局走多少步\n",
    "        env.render() # 用于渲染出当前的智能体以及环境的状态\n",
    "        # In each state, we select the action by epsilon-greedy policy\n",
    "        action = epsilon_greedy_policy(prev_state, epsilon)\n",
    "        # then we perform the action and move to the next state, and receive the reward\n",
    "        nextstate, reward, done, _ = env.step(action)\n",
    "        # Next we update the Q value using our update_q_table function\n",
    "        # which updates the Q value by Q learning update rule\n",
    "        update_q_table(prev_state, action, reward, nextstate, alpha, gamma)\n",
    "        # Finally we update the previous state as next state\n",
    "        prev_state = nextstate # s <- s'\n",
    "        # Store all the rewards obtained\n",
    "        r += reward # reward: 即时奖励, r: total reward\n",
    "        # we will break the loop, if we are at the terminal state of the episode\n",
    "        if done:\n",
    "            break\n",
    "    print(f\"Episode: {episode + 1}\") # 玩几局游戏\n",
    "    print(f\"Epochs: {steps}\") # 每局走多少步\n",
    "    print(f\"State: {prev_state}\")\n",
    "    print(f\"Action: {action}\")\n",
    "    print(f\"Reward: {reward}\")\n",
    "    print(\"Total Reward: \", r)\n",
    "    # sleep(0.01) # 为了让显示变慢，否则画面会非常快\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
